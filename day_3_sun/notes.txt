Pytest is a python testing framework. it is better than unitest that ships with python as it reduces boiler plate codes and makes testing easier and more efficient.

Most functional tests follow the Arrange-Act-Assert model:

- Arrange, or set up, the conditions for the test
- Act by calling some function or method
- Assert that some end condition is true

sameple code 

def test_always_passes():
    assert True

def test_always_fails():
    assert False


The beauty of `pytest` is that it completely hijacks Python's built-in `assert` keyword and gives it super-powers. In older frameworks (like `unittest`), you had to memorize dozens of methods like `assertEqual`, `assertTrue`, `assertIn`.

With Pytest, you just use standard Python math and logic.

For a $5k/month Data Engineering role, you don't just test if `1 + 1 == 2`. You test if your data pipelines are transforming records correctly, handling bad data, and doing math accurately.

Here is your DE-focused cheat sheet for assertions:

### 1. The Core Operators (Your Daily Drivers)

You will use these to check the output of your data transformation functions.

* **Equality (`==`)**: Did my function output the exact dictionary or list I expected?
```python
def test_clean_user_record():
    raw_data = {"Name": " ALICE ", "age": "25"}
    # Assert the pipeline cleans the whitespace and casts the integer
    assert clean_data(raw_data) == {"name": "alice", "age": 25}

```


* **Membership (`in` / `not in`)**: Did my pipeline successfully add (or drop) a specific column/key?
```python
def test_pii_dropped():
    secure_record = mask_sensitive_data({"user": "Bob", "ssn": "123-45"})
    # Assert the social security number was removed
    assert "ssn" not in secure_record 

```


* **Length and Type (`len()`, `isinstance()`)**: Did the API pagination function pull the correct number of records?
```python
def test_api_pull():
    records = fetch_page(page_number=1)
    assert len(records) == 100
    assert isinstance(records, list)

```



### 2. The Float Problem: `pytest.approx` (Crucial for DE)

In Data Engineering, you often calculate averages or aggregate financial data. Python has a famous quirk with floating-point math: `0.1 + 0.2` actually equals `0.30000000000000004`.
If you write `assert 0.1 + 0.2 == 0.3`, **your test will fail.**

You must use `approx` for math tests to allow for microscopic rounding differences.

```python
from pytest import approx

def test_average_revenue():
    avg = calculate_average_revenue([0.1, 0.2])
    assert avg == approx(0.3)

```

### 3. Testing for Failure: `pytest.raises`

A senior DE doesn't just test the "happy path." You must prove that your pipeline securely crashes and throws the correct error when a corrupted CSV or bad API payload hits it.

You use a context manager (`with`) to assert that an error *will* happen.

```python
import pytest
from pydantic import ValidationError

def test_pipeline_rejects_bad_data():
    bad_record = {"age": "twenty"} # Should be an int
    
    # Assert that the code inside this block throws a ValidationError
    with pytest.raises(ValidationError):
        User.model_validate(bad_record)

```

### 4. Custom Error Messages (The Debugging Lifesaver)

When a pipeline fails in an automated CI/CD system (like GitHub Actions), you want the logs to tell you exactly *why* it failed without having to dig through code. You can attach a custom message to any assert by adding a comma.

```python
def test_database_connection():
    status = check_db()
    assert status == "connected", f"Expected connected, but DB returned {status}"

```

### Advanced Pytest Assertions for Data Engineering

**1. Pytest Introspection (Automatic Diffs)**

* **Concept:** You don't need special methods to compare massive dictionaries or lists. Pytest automatically intercepts standard `==` assertions and prints a highly detailed, color-coded diff showing the exact missing key or mismatched value.
* **Usage:** Just use `assert dict_A == dict_B`.

**2. The Subset Match (API / JSON testing)**

* **Concept:** Use the subset operator (`<=`) to verify that specific key-value pairs exist inside a massive dictionary without having to mock the entire 50-key payload.
* **Code:**
```python
expected_subset = {"is_active": True, "risk_score": 0.5}
actual_record = get_enriched_user(105) # Returns a massive dict

assert expected_subset.items() <= actual_record.items()

```



**3. Asserting Exact Error Messages**

* **Concept:** Don't just test that an error happened; test that it happened for the *right reason* by capturing the exception and checking its string value.
* **Code:**
```python
import pytest

with pytest.raises(ValueError) as exc_info:
    process_csv_row({"name": "Alice"}) # Missing 'age'

assert "Missing required column: age" in str(exc_info.value)

```



**4. The DataFrame Trap**

* **Concept:** Standard `assert df1 == df2` will violently crash with an "ambiguous truth value" error. You must use the built-in testing tools for Pandas or PySpark when comparing grid data.
* **Code:**
```python
from pandas.testing import assert_frame_equal 

# Do this instead of standard assert
assert_frame_equal(clean_df, expected_df) 

```



**5. String Contains (SQL Generation)**

* **Concept:** When testing dynamically generated SQL queries, never use `==` because invisible spaces, tabs, or newlines will cause false failures. Check for the presence of keywords using `in`.
* **Code:**
```python
query = build_select_query("users", limit=100)

assert "SELECT *" in query
assert "LIMIT 100" in query

```

=================================

      PYTEST FIXTURES

=================================

pytest fixtures are functions that can create data, test doubles, or initialize system state for the test suite. Any test that wants to use a fixture must explicitly use this fixture function as an argument to the test function, so dependencies are always stated up front:

import pytest

@pytest.fixture
def example_fixture():
    return 1

def test_with_fixture(example_fixture):
    assert example_fixture == 1


### Pytest Fixtures & Architecture (Data Engineering Focus)

**1. The Core Concept: Dependency Injection**

* **What it is:** Fixtures replace the old `setUp()` methods. They are reusable functions decorated with `@pytest.fixture` that generate data, connections, or state.
* **How to use it:** Pass the exact name of the fixture as an argument to your test function. Pytest will run the fixture and inject the result.

**2. Setup and Teardown (`yield`)**

* **Concept:** To prevent memory leaks or locked databases, fixtures must clean up after themselves, even if a test crashes.
* **Syntax:** Use `yield` instead of `return`. Everything before `yield` is the setup; everything after `yield` is the teardown.
```python
@pytest.fixture
def db_connection():
    conn = connect_to_db() # SETUP
    yield conn             # HAND TO TEST
    conn.close()           # TEARDOWN (Always runs)

```



**3. Fixture Scope (Performance Optimization)**

* **Concept:** Prevents heavy setups (like spinning up Apache Spark or a Docker DB) from running for every single test.
* **Settings:** `@pytest.fixture(scope="...")`
* `"function"` (Default): Runs from scratch for every test. (Use for small DataFrames/dicts).
* `"module"`: Runs once per Python file.
* `"session"`: Runs once per entire test suite run. (Use for heavy database connections).



**4. Built-in "Super Fixtures" for DE**
You don't write these; just request them by name in your test.

* **`tmp_path`**: Creates a unique, safe, auto-deleting temporary folder for testing file I/O (CSV/Parquet exports) so parallel tests don't overwrite each other.
* **`monkeypatch`**: Safely overrides environment variables (like `AWS_ACCESS_KEY` or `DATABASE_URL`) just for the duration of the test, ensuring you never accidentally hit production.

**5. Architecture: `conftest.py**`

* **Concept:** A magical file in the root of your test folder. Any fixture placed inside `conftest.py` becomes globally available to all your test files.
* **Benefit:** You never have to `import` fixtures. Pytest finds them automatically, keeping your test files incredibly clean.

**6. Fixture Chaining (Advanced Orchestration)**

* **Concept:** Fixtures can request *other* fixtures as arguments. This allows you to build a modular assembly line before the test even begins.
* **Example Pipeline:** 1. Fixture A creates an empty RAM database (`:memory:`).
2. Fixture B requests Fixture A, inserts messy CSV data into it, and yields the populated database.
3. The Test requests Fixture B, running the pipeline logic on a perfectly prepared environment.

---


Scaling & Organizing Pytest (Data Engineering Focus)

Avoid Over-Fixturing: Do not create new fixtures just to change one value in a dataset. Use fixtures for infrastructure (DBs, files), and use Parametrization for data variations.

conftest.py (The Infrastructure Hub): A special file where you place fixtures. Any fixture inside conftest.py is globally and automatically available to all tests in that folder without needing to be imported.

autouse=True (The Invisible Guard): Forces a fixture to run for every test automatically.

Pro-DE Tip: Use autouse=True alongside monkeypatch to block all external network/database calls globally. This guarantees no test ever hits the production environment.

Marks (@pytest.mark.tag_name): Tags applied to test functions allowing you to categorize them (e.g., @pytest.mark.slow, @pytest.mark.spark).

Execution: Run specific tags from the terminal using pytest -m "slow" or exclude them using pytest -m "not slow". Essential for skipping heavy database tests during local development.

Built-in Marks: @pytest.mark.skip (always ignore this test) and @pytest.mark.xfail (this test is expected to fail because of a known bug).

The "Incomprehensibility" Warning (Crucial DE Tip):
The text warns that by combining 7 tests into 1, you lose the "descriptive nature" of the original function names. If a test fails, Pytest will just output a generic failure, making debugging hard.

You fix this by using the ids parameter to label exactly what each piece of data is testing:

Python
# A real-world DE example: Testing a data cleaner
@pytest.mark.parametrize("raw_input, expected_output", [
    (" ALICE ", "alice"),
    (None, "unknown"),
], ids=["handles_whitespace", "handles_null_values"]) # <-- The Fix
def test_name_cleaner(raw_input, expected_output):
    assert clean_name(raw_input) == expected_output
Now, if the test fails on a null value, your terminal will clearly state: FAILED test_name_cleaner[handles_null_values].