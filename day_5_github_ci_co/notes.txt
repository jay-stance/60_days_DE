GitHub Actions is a continuous integration and continuous delivery (CI/CD) platform that allows you to automate your build, test, and deployment pipeline.

A workflow is a configurable automated process that will run one or more jobs. Workflows are defined by a YAML file checked in to your repository and will run when triggered by an event in your repository, or they can be triggered manually, or at a defined schedule.

Workflows are defined in the .github/workflows directory in a repository. A repository can have multiple workflows, each of which can perform a different set of tasks such as:

Building and testing pull requests
Deploying your application every time a release is created
Adding a label whenever a new issue is opened

An event is a specific activity in a repository that triggers a workflow run. For example, an activity can originate from GitHub when someone creates a pull request, opens an issue, or pushes a commit to a repository. 


A job is a set of steps in a workflow that is executed on the same runner. Each step is either a shell script that will be executed, or an action that will be run. Steps are executed in order and are dependent on each other.

An action is a pre-defined, reusable set of jobs or code that performs specific tasks within a workflow, reducing the amount of repetitive code you write in your workflow files. Actions can perform tasks such as:

Pulling your Git repository from GitHub
Setting up the correct toolchain for your build environment
Setting up authentication to your cloud provider

A runner is a server that runs your workflows when they're triggered. Each runner can run a single job at a time. GitHub provides Ubuntu Linux, Microsoft Windows, and macOS runners to run your workflows. Each workflow run executes in a fresh, newly-provisioned virtual machine.


The GitHub Actions Hierarchy (The Russian Nesting Doll)
Think of GitHub Actions like an automated factory.

    The Event (The Trigger): The factory alarm goes off. ("Hey, Jay just pushed new code to the main branch!")

    The Workflow (The Factory Manager): The manager wakes up, looks at the .yml file you wrote, and says, "Okay, I see the instructions. I need to run the testing phase, and if that passes, the deployment phase."

    The Runner (The Workbench): GitHub spins up a blank, temporary Ubuntu Linux server just for you. This is where the actual work will happen.

    The Job (The Phase): A major grouping of work. For example, "Test the Code" is one Job. "Deploy the Docker Container" is a second Job. Each Job gets its own Runner (Workbench).

    The Step (The To-Do List): Inside a Job, you have a chronological list of things to do. "Step 1: Download Python. Step 2: Install dependencies. Step 3: Run Pytest."

    The Action (The Power Tool): Instead of writing 50 lines of bash script to download your code into the Runner, you use a pre-built "Action" that someone else wrote. It's a plug-and-play shortcut.

The Golden Rule of the Hierarchy:Events trigger Workflows $\rightarrow$ which contain Jobs $\rightarrow$ which execute on Runners $\rightarrow$ and are made of Steps $\rightarrow$ which use pre-built Actions or custom shell scripts.


That means, a typical github actins template file would look like this 

Name: workflow name 

on: 
  ... which events should trigger it 
  push:
    [branches]
  pull_request:

jobs:
  job_name
    runs-on: ubuntu-latest

    steps: ..... the steps in the job, remb each job runs in its own runner 
      name: install dependencies
      run: pip install -r requirements.txt
      uses: actions/checkout@v4 ... downloads and uses a predefined action 


name: CI Pipeline

on:
  push:
    branches: [ main ]
  pull_request:

jobs:
  test-and-build:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        run: |
          sudo apt update
          sudo apt install -y python3 python3-pip

      - name: Install dependencies
        run: pip3 install -r requirements.txt

      - name: Run tests
        run: pytest

      - name: Build Docker image
        run: docker build -t myapp:${{ github.sha }} .


Understanding ${{ }} Expressions
GitHub uses expression syntax:

${{ github.sha }}

This accesses context variables.

Common contexts:

    github.sha
    github.ref
    github.event_name
    secrets.MY_SECRET
      Defined in:
      Repo → Settings → Secrets
    env.VAR_NAME

Job Dependencies

Jobs can depend on each other.

Example:

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - run: echo "Testing"

  build:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - run: echo "Building"

Matrix lets you define a template once and generate multiple jobs automatically.

Basic Matrix Example
jobs:
  test:
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]

    steps:
      - name: Print version
        run: echo "Testing on ${{ matrix.python-version }}"

What Actually Happens Internally

GitHub does this behind the scenes:
It expands this into:

  Job 1:
  python-version = 3.9

  Job 2:
  python-version = 3.10

  Job 3:
  python-version = 3.11

Inside the job, GitHub injects a variable called matrix.

Important: Matrix is Job-Level, Not Step-Level
The entire job is duplicated.
Not just the step.
Everything inside the job runs multiple times.

Multi-Dimensional Matrix

jobs:
  test:
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest]
        python-version: [3.9, 3.10]

    runs-on: ${{ matrix.os }}

    steps:
      - name: Show combination
        run: echo "OS: ${{ matrix.os }}, Python: ${{ matrix.python-version }}"

GitHub generates all combinations.

This is called Cartesian product.

You get:

ubuntu + 3.9
ubuntu + 3.10
windows + 3.9
windows + 3.10

So 2 × 2 = 4 jobs.

Each combination runs separately.

if one job in the matrix fails, every other one fails, unless you specify otherwise
strategy:
  fail-fast: false
  matrix:
    python-version: [3.9, 3.10, 3.11]

Advanced: Excluding Combinations

Sometimes you don’t want every combination.

Example:

strategy:
  matrix:
    os: [ubuntu-latest, windows-latest]
    python-version: [3.9, 3.10]
    exclude:
      - os: windows-latest
        python-version: 3.9

Now Windows + 3.9 won’t run.

You control exact combinations.

Artifacts are used to store and pass files across different jobs in github workflow. cod ordinarily ever job clears all its state before entering the next job. but with artifcats we can pass a file eg report or build wheels from one job to another

ARTIFACTS IN ACTION 

name: Python Data Pipeline CI

on:
  push:
    branches: [ "main" ]

jobs:
  # ==========================================
  # JOB 1: THE BUILDER (Runs once)
  # ==========================================
  build_package:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Compile heavy dependencies into a Wheel
        run: |
          pip install wheel
          # This command compiles the code and saves it in a folder called 'dist/'
          pip wheel . --wheel-dir=dist/ 

      - name: Upload the Wheel Artifact
        uses: actions/upload-artifact@v4
        with:
          name: compiled-wheel-files  # We name the USB drive
          path: dist/                 # We upload the whole 'dist' folder


  # ==========================================
  # JOB 2: THE MATRIX (Runs 3 times in parallel)
  # ==========================================
  test_matrix:
    # THE MAGIC: This forces the matrix to wait for Job 1 to finish successfully!
    needs: build_package 
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.9', '3.10', '3.11']

    steps:
      - name: Checkout Code (Need the test files)
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      # THE RETRIEVAL: We plug the virtual USB drive into this new server
      - name: Download the Wheel Artifact
        uses: actions/download-artifact@v4
        with:
          name: compiled-wheel-files  # Must match the exact name from Job 1
          path: downloaded-wheels/    # Where to save it on this new server

      - name: Install the pre-compiled Artifact and Run Tests
        run: |
          # Install the .whl file we downloaded instead of building from scratch
          pip install downloaded-wheels/*.whl
          
          # Run the actual tests
          pytest tests/


Environment Variables

You can define:

Global:

env:
  APP_ENV: production

Or per job:

jobs:
  build:
    env:
      DOCKER_TAG: latest


CREATING CUSTOM ACTION 

custom action template looks like this 

# 1. The Metadata (Required)
name: 'My Awesome Custom Action'
description: 'A brief explanation of what this tool does.'
author: 'Your Name or Company'

# 2. The Parameters / Arguments (Optional)
inputs:
  my_input_variable:
    description: 'What this variable does'
    required: true    # If true, the workflow crashes if the user forgets to provide it
    default: 'prod'   # A fallback value if the user leaves it blank

# 3. The Return Values (Optional)
outputs:
  my_output_variable:
    description: 'The result generated by this action to be used in later steps'

# 4. The Execution Engine (Required)
runs:
  # ... This part changes depending on the TYPE of action ...

  Template A: The Composite Action (Bash/Shell)
  Use this when you just want to group standard Linux commands (like pip install, curl, or aws s3 cp) together.

  YAML
  runs:
    using: "composite"
    steps:
      - name: Do a thing
        run: echo "Hello ${{ inputs.my_input_variable }}"
        shell: bash   # CRITICAL: In composite actions, you MUST specify the shell for every run step!

      - name: Set an output variable
        # This weird syntax is how you tell GitHub to save a return value
        run: echo "my_output_variable=Success" >> $GITHUB_OUTPUT
        shell: bash

      
  Template B: The Docker Action (Python)
  Use this when your logic is too complex for Bash and requires a real programming language and a Dockerfile.

  YAML
  runs:
    using: 'docker'
    image: 'Dockerfile' # Tells GitHub to build the Dockerfile sitting next to this action.yml
    
    # How you pass the inputs into the Docker container
    env:
      # Maps the GitHub input to a Linux environment variable inside the container
      INPUT_VAR: ${{ inputs.my_input_variable }} 
    
    args:
      # Alternatively, pass inputs as command-line arguments to your script
      - ${{ inputs.my_input_variable }}



The Folder Structure
To make this work, your repository needs to look exactly like this:

Plaintext
your_repo/
├── .github/
│   ├── workflows/
│   │   └── data_pipeline.yml            # The main workflow file
│   └── actions/
│       ├── composite-data-fetcher/      # Custom Action 1 (Composite)
│       │   └── action.yml
│       └── python-data-validator/       # Custom Action 2 (Docker/Python)
│           ├── action.yml
│           ├── Dockerfile
│           ├── requirements.txt
│           └── validate.py
└── test_data/                           # Where the composite action saves data
Custom Action 1: The Composite Action (The Fetcher)
This action is simple. We use it to group standard Linux shell commands together. We will define an "Input" so we can tell it exactly which URL to download data from.

.github/actions/composite-data-fetcher/action.yml

YAML
name: 'Fetch Test Data'
description: 'Downloads sample data using curl'

# We define an input variable just like a Python function argument
inputs:
  dataset_url:
    description: 'The URL of the CSV file to download'
    required: true

runs:
  using: "composite"
  steps:
    - name: Create directory
      run: mkdir -p test_data
      shell: bash

    - name: Download the data
      # We use the input variable provided by the user
      run: curl -sL ${{ inputs.dataset_url }} -o test_data/sample.csv
      shell: bash
      
    - name: Confirm download
      run: ls -la test_data/
      shell: bash
Custom Action 2: The Python Action (The Validator)
This is the heavy lifter. We are going to package a Python script into a Docker container and tell GitHub to use it as an Action.

1. The Action Definition (.github/actions/python-data-validator/action.yml)
Notice that using is set to docker, and we tell it to look at the Dockerfile right next to it.

YAML
name: 'Python Data Schema Validator'
description: 'Validates a CSV file using Pandas'

inputs:
  file_path:
    description: 'Path to the CSV file to validate'
    required: true

runs:
  using: 'docker'
  image: 'Dockerfile'
2. The Dockerfile (.github/actions/python-data-validator/Dockerfile)
This is a standard Dockerfile. It copies the Python script and installs Pandas.

Dockerfile
FROM python:3.11-slim
# Install pandas
COPY requirements.txt .
RUN pip install -r requirements.txt

# Copy our custom validation script
COPY validate.py /validate.py

# When the action runs, execute the script
ENTRYPOINT ["python", "/validate.py"]
3. The Python Script (.github/actions/python-data-validator/validate.py)
The Magic Connection: When GitHub Actions passes an input to a Docker Action, it injects it into the container as an Environment Variable prefixed with INPUT_. So, our input file_path becomes INPUT_FILE_PATH.

Python
import os
import pandas as pd
import sys

# 1. Grab the input passed from the GitHub Workflow
file_path = os.environ.get("INPUT_FILE_PATH")

print(f"Starting Data Validation for: {file_path}")

try:
    df = pd.read_csv(file_path)
    
    # 2. Our custom Data Engineering logic
    required_columns = ['user_id', 'email', 'transaction_total']
    
    for col in required_columns:
        if col not in df.columns:
            print(f"CRITICAL ERROR: Missing required column '{col}'")
            sys.exit(1) # This tells GitHub Actions to FAIL the step!
            
    print("SUCCESS: Data schema is perfectly valid!")
    sys.exit(0) # This tells GitHub Actions the step PASSED.

except Exception as e:
    print(f"Failed to read data: {e}")
    sys.exit(1)
Bringing It All Together: The Main Workflow
Now, look at how incredibly clean your actual CI/CD pipeline becomes. You don't have messy curl commands or inline Python scripts cluttering up the main file. You just call the custom tools you built.

.github/workflows/data_pipeline.yml

YAML
name: Nightly Data Validation Pipeline

on:
  push:
    branches: [ "main" ]

jobs:
  validate_data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      # 1. Call your Custom Composite Action
      - name: Get the latest test dataset
        uses: ./.github/actions/composite-data-fetcher
        with:
          # We pass the URL into the input we defined!
          dataset_url: 'https://raw.githubusercontent.com/some-repo/data.csv'

      # 2. Call your Custom Python/Docker Action
      - name: Validate Schema with Python
        uses: ./.github/actions/python-data-validator
        with:
          # We pass the file path we just downloaded
          file_path: 'test_data/sample.csv'
          
      - name: Final Step
        run: echo "If the Python script failed, this line will never run!"
The Power of the Setup
By writing it this way, your Python logic is completely isolated. You can test validate.py locally on your laptop without needing GitHub Actions. If your team creates a second repository that also needs CSV validation, you don't rewrite the code; you just point the new workflow to this exact same custom action.